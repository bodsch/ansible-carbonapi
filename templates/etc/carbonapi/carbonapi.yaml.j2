# {{ ansible_managed }}

# Need to be URL, http or https
# This url specifies the backend or a loadbalancer
#
# Is you are using carbonzipper you should set it to
# zipper's url
#

listen: "{{ carbonapi_listen }}"

# Specify URL Prefix for all handlers
prefix: ""

# Specify if metrics are exported over HTTP and if they are available on the same address or not
# pprofEnabled controls if extra HTTP Handlers to profile and debug application will be available
expvar:
  enabled: true
  pprofEnabled: false
  listen: ""

# Allow extra charsets in metric names. By default only "Latin" is allowed
# Please note that each unicodeRangeTables will slow down metric parsing a bit
#   For list of supported tables, see: https://golang.org/src/unicode/tables.go?#L3437
#   Special name "all" reserved to append all tables that's currently supported by Go
#unicodeRangeTables:
#   - "Latin"
#   - "Cyrillic"
#   - "Hiragana"
#   - "Katakana"
#   - "Han"
##   - "all"

# Controls headers that would be passed to the backend
headersToPass:
  - "X-Dashboard-Id"
  - "X-Grafana-Org-Id"
  - "X-Panel-Id"
headersToLog:
  - "X-Dashboard-Id"
  - "X-Grafana-Org-Id"
  - "X-Panel-Id"

# Specify custom function aliases.
#  This is example for alias "perMinute(metrics)" that will behave as "perSecond(metric)|scale(60)"
# define:
#   - name: "perMinute"
#     template: "perSecond(\{\{.argString}}) | scale(60)"

# Control what status code will be returned where /render or find query do not return any metric
notFoundStatusCode: 404

# Max concurrent requests to CarbonZipper
concurency: 1000

cache:
  type: "{{ carbonapi_cache.type }}"
  size_mb: {{ carbonapi_cache.size_mb | int }}
  defaultTimeoutSec: {{ carbonapi_cache.default_timeout | int }}
{% if carbonapi_cache.type == "memcache" %}
  memcachedServers:
{% if carbonapi_cache.memcached_servers is defined and
      carbonapi_cache.memcached_servers | type == "list" %}
{% for host in carbonapi_cache.memcached_servers %}
    - "{{ host }}"
{% endfor %}
{% endif %}
{% endif %}

# Amount of CPUs to use. 0 - unlimited
cpus: {{ carbonapi_cpu | int }}

# Timezone, default - local
tz: ""

# For go-carbon --- it depends on how you use it.
sendGlobsAsIs: true

functionsConfig:
  graphiteWeb: /etc/carbonapi/graphiteWeb.yaml
  timeShift: /etc/carbonapi/timeShift.yaml

maxBatchSize: 100

graphite:
  host: "{{ carbonapi_graphite.host }}"
  interval: "{{ carbonapi_graphite.interval }}"
  prefix: "{{ carbonapi_graphite.prefix }}"
  pattern: "{{ carbonapi_graphite.pattern }}"

# Configures how often keep alive packets will be sent out
keepAliveInterval: "30s"

# Config to ensure we return version needed for providing integrated graphite docs in grafana
# without supporting tags
graphiteVersionForGrafana: 1.1.0

# Maximium idle connections to carbonzipper
idleConnections: 10
pidFile: ""

# See https://github.com/go-graphite/carbonzipper/blob/master/example.conf#L70-L108 for format explanation
# https://github.com/go-graphite/carbonapi/blob/main/cmd/carbonapi/carbonapi.example.yaml
upstreams:
  tldCacheDisabled: false

  # Number of 100ms buckets to track request distribution in. Used to build
  # 'carbon.zipper.hostname.requests_in_0ms_to_100ms' metric and friends.
  # Requests beyond the last bucket are logged as slow (default of 10 implies
  # "slow" is >1 second).
  # The last bucket is _not_ called 'requests_in_Xms_to_inf' on purpose, so
  # we can change our minds about how many buckets we want to have and have
  # their names remain consistent.
  buckets: 10

  timeouts:
    find: "2s"
    render: "10s"
    connect: "200ms"

  # Number of concurrent requests to any given backend - default is no limit.
  # If set, you likely want >= MaxIdleConnsPerHost
  concurrencyLimitPerServer: 0

  # Configures how often keep alive packets will be sent out
  keepAliveInterval: "30s"

  # Control http.MaxIdleConnsPerHost. Large values can lead to more idle
  # connections on the backend servers which may bump into limits; tune with care.
  maxIdleConnsPerHost: 100

  # Only affects cases with maxBatchSize > 0.
  # If set to `false` requests after split will be sent out one by one, otherwise in parallel
  doMultipleRequestsIfSplit: false

#   backends:
# {% if carbonapi_upstreams_backends is defined and carbonapi_upstreams_backends | count != 0 %}
# {% for host in carbonapi_upstreams_backends %}
#     - "{{ host }}"
# {% endfor %}
# {% endif %}

  # backends section will override this one!
  backendsv2:
    backends:
      - groupName: "group1"
        # supported:
        #    carbonapi_v2_pb - carbonapi 0.11 or earlier version of protocol.
        #    carbonapi_v3_pb - new protocol, http interface (native)
        #    carbonapi_v3_grpc - new protocol, gRPC interface (native)
        #    protobuf, pb, pb3 - same as carbonapi_v2_pb
        #    msgpack - protocol used by graphite-web 1.1 and metrictank
        #    auto - carbonapi will do it's best to guess if it's carbonapi_v3_pb or carbonapi_v2_pb
        #
        #  non-native protocols will be internally converted to new protocol, which will increase memory consumption
        protocol: "auto"

        # supported:
        #    "broadcast" - send request to all backends in group and merge responses. This was default behavior for carbonapi 0.11 or earlier
        #    "roundrobin" - send request to one backend.
        #    "all - same as "broadcast"
        #    "rr" - same as "roundrobin"
        lbMethod: "broadcast"

        # amount of retries in case of unsuccessful request
        maxTries: 3

        # amount of metrics per fetch request. Default: 0 - unlimited. If not specified, global will be used
        maxBatchSize: 100

        # interval for keep-alive http packets. If not specified, global will be used
        keepAliveInterval: "10s"

        # override for global concurrencyLimit.
        concurrencyLimit: 0

        # override for global maxIdleConnsPerHost
        maxIdleConnsPerHost: 1000

        # force attempt to establish HTTP2 connection, instead of http1.1. Default: false
        # Backends must use https for this to take any effect
        forceAttemptHTTP2: false

        # Only affects cases with maxBatchSize > 0. If set to `false` requests after split will be sent out one by one, otherwise in parallel
        doMultipleRequestsIfSplit: false

        # per-group timeout override. If not specified, global will be used.
        # Please note that ONLY min(global, local) will be used.
        timeouts:
          # Maximum backend request time for find requests.
          find: "2s"
          # Maximum backend request time for render requests. This is total one and doesn't take into account in-flight requests.
          render: "50s"
          # Timeout to connect to the server
          connect: "200ms"
        servers:
{% if carbonapi_upstreams_backends is defined and carbonapi_upstreams_backends | count != 0 %}
{% for host in carbonapi_upstreams_backends %}
          - "{{ host }}"
{% endfor %}
{% endif %}
          # - "http://127.0.0.2:8080"
          # - "http://127.0.0.3:8080"

      # - groupName: "group2"
      #   protocol: "carbonapi_v3_pb"
      #   lbMethod: "roundrobin"
      #   servers:
      #       - "http://127.0.0.4:8080"
      #       - "http://127.0.0.5:8080"

  # Enable compatibility with graphite-web 0.9
  # This will affect graphite-web 1.0+ with multiple cluster_servers
  # Default: disabled
  graphite09compat: false

graphTemplates: {{ carbonapi_graph_templates }}
expireDelaySec: {{ carbonapi_expire_delay | int }}

logger:
{% for logger,v  in carbonapi_logger.items() %}
{% set l = carbonapi_logger[logger] %}
{% set _logger =  l['logger'] %}
{% set _ = l.pop('logger') %}
  - logger: {{ _logger  }}
{% for k, v in l.items() %}
    {{ k }}: "{{ v }}"
{% endfor %}
{% endfor %}

# tagdb:
#   url: "http://127.0.0.1:9090"

monitoring:
  timeInQueueExpHistogram:
    start: 0.05
    bucketsNum: 25
    bucketSize: 2
  requestDurationExpHistogram:
    start: 0.05
    bucketsNum: 20
    bucketSize: 2
  requestDurationLinHistogram:
    start: 0.0
    bucketsNum: 40
    bucketSize: 0.05
  findDurationExpHistogram:
    start: 0.001
    bucketsNum: 20
    bucketSize: 2
  findDurationLinHistogram:
    start: 0.5
    bucketsNum: 20
    bucketSize: 0.5
